{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d464847c-7589-4662-acea-86f4ea52c636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f47ce482-4874-454f-99a3-443b3a5bab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus  = \"\"\" Hello Welcome, to Rahmat's Tokenization Practicals.\n",
    "Please! look into the following code.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f2f3f5-c51f-4c38-98cd-034f0de129ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Welcome, to Rahmat's Tokenization Practicals.\n",
      "Please! look into the following code\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73cc5f5-6089-4519-8daa-a27537baeb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rahmatunnisakhatoon/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization\n",
    "## Sentence --> paragraphs\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96081587-a8ab-41a0-9c66-7ad2bf0bce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ecf4e0b-f397-462d-bc5c-1032666b12c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba93e97d-e8d6-45ac-9535-621b2c759b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Welcome, to Rahmat's Tokenization Practicals.\n",
      "Please!\n",
      "look into the following code\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebdbbc7f-5423-41ce-8469-4e2b790b07d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization\n",
    "##Paragraph --> words\n",
    "##sentence --> words\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3f6bce-d0ef-4c98-a982-7d7316737448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Rahmat', \"'s\", 'Tokenization', 'Practicals', '.', 'Please', '!', 'look', 'into', 'the', 'following', 'code']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddf2cb37-732d-48f3-954c-3be16f29481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ed381d9-2e89-49f0-853b-e8244f12a255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Rahmat',\n",
       " \"'\",\n",
       " 's',\n",
       " 'Tokenization',\n",
       " 'Practicals',\n",
       " '.',\n",
       " 'Please',\n",
       " '!',\n",
       " 'look',\n",
       " 'into',\n",
       " 'the',\n",
       " 'following',\n",
       " 'code']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1bd93a6-eb16-4564-b8be-19a48d2199b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4395e650-d3b5-4bc5-b5be-1d1e10d4c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e9c4c51-de2b-40e3-8887-49ef9b019076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Rahmat',\n",
       " \"'s\",\n",
       " 'Tokenization',\n",
       " 'Practicals.',\n",
       " 'Please',\n",
       " '!',\n",
       " 'look',\n",
       " 'into',\n",
       " 'the',\n",
       " 'following',\n",
       " 'code',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32145341-32a0-40f7-92af-35e299bb3b1a",
   "metadata": {},
   "source": [
    "### PortStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21ab3125-075a-4319-a870-8ae52e000d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4088b1c4-cba3-460e-885a-2d3044c97d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82b17c57-86c8-41ff-ab26-36eff6ce203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d379a8ce-ea84-465b-b32a-ce0b9bc19e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eats\", \"eaten\",\"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d8ca981-60b8-4e7a-aea0-81667aea887a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eats--->eat\n",
      "eaten--->eaten\n",
      "writing--->write\n",
      "writes--->write\n",
      "programming--->program\n",
      "programs--->program\n",
      "history--->histori\n",
      "finally--->final\n",
      "finalize--->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"--->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdb55826-6d09-4055-9e11-ca4c70706e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Regexp Stemmer\n",
    "\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "599334eb-1f2f-4a60-8dbe-763dc1a24685",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing|s$|e$|able$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95bef1d9-a31b-48e6-a3ce-ad9a0ba12eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af91a986-125b-49af-a53d-f10230b2fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ffe5b-d2e1-4912-b57f-f36b466c110f",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fcd09fa-3d35-4cee-829b-c367cbd7aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e825e33-f127-4220-b3e8-6d4c52d2609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebf11d0f-9e37-4c58-91b1-6a4c773518c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --->eat\n",
      "eats --->eat\n",
      "eaten --->eaten\n",
      "writing --->write\n",
      "writes --->write\n",
      "programming --->program\n",
      "programs --->program\n",
      "history --->histori\n",
      "finally --->final\n",
      "finalize --->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \" --->\" + snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13852699-8407-4aef-969f-83b5e1982aa1",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12306884-d47a-4ec0-b0ce-b72ca6d10f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rahmatunnisakhatoon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Q&A, chatbots, text summerization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ca0dca5-f15f-4251-9a7f-c2dddb4d5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3edd98e0-91c5-4559-94d2-abb48a2ea36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\",pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7bce8243-f0a4-4750-b001-87449777cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eats--->eat\n",
      "eaten--->eat\n",
      "writing--->write\n",
      "writes--->write\n",
      "programming--->program\n",
      "programs--->program\n",
      "history--->history\n",
      "finally--->finally\n",
      "finalize--->finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"--->\"+lemmatizer.lemmatize(word,pos ='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb830d2-c6f0-40b7-a554-318466b643a6",
   "metadata": {},
   "source": [
    "### Stopwords with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9bed3453-4ed8-4b06-b072-c9f97d17fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speech of Lincoln \n",
    "paragraph = \"\"\" Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\n",
    "\n",
    "Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.\n",
    "\n",
    "But, in a larger sense, we can not dedicate—we can not consecrate—we can not hallow—this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced.\n",
    "\n",
    "It is rather for us to be here dedicated to the great task remaining before us—that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in vain—that this nation, under God, shall have a new birth of freedom—and that government of the people, by the people, for the people, shall not perish from the earth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "567c1ad1-ed2c-40dd-9663-cca3db9d70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d372e823-c2cf-430c-9d4c-c664ad0bc3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rahmatunnisakhatoon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28d16272-12d8-4dba-b808-d5d320b409c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b3ade86-7c0c-4448-aeff-f1d9ee7d87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1862c45e-237e-4867-bece-16ab0f20671f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c3f5fc5-d1dc-42d4-b0d7-a870b55c2396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2a41e243-2993-4cab-9c9d-6e4fa5615c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Applying Stopwords and Filter and then Apply Stemming\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) #converting words into sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94de7a13-8e25-40a2-bdc5-11a217d166be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['four score seven year ago father brought forth contin , new nation , conceiv liberti , dedic proposit men creat equal .',\n",
       " 'now engag great civil war , test whether nation , nation conceiv dedic , long endur .',\n",
       " 'we met great battle-field war .',\n",
       " 'we come dedic portion field , final rest place gave live nation might live .',\n",
       " 'it altogeth fit proper .',\n",
       " 'but , larger sens , dedicate—w consecrate—w hallow—thi ground .',\n",
       " 'the brave men , live dead , struggl , consecr , far poor power add detract .',\n",
       " 'the world littl note , long rememb say , never forget .',\n",
       " 'it us live , rather , dedic unfinish work fought thu far nobli advanc .',\n",
       " 'it rather us dedic great task remain us—that honor dead take increas devot caus gave last full measur devotion—that highli resolv dead shall die vain—that nation , god , shall new birth freedom—and govern peopl , peopl , peopl , shall perish earth .']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a9d889c-27e4-4bcd-abdb-9f87bff00a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To have more refined words using Snowball Stemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4f3bb5e-419f-453f-81b5-be949a45a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2450e4dd-8700-426c-af62-9414feb382a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) #converting words into sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7947e23c-9f1b-452e-b43c-7f7101b44649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['four score seven year ago father brought forth contin , new nation , conceiv liberti , dedic proposit men creat equal .',\n",
       " 'engag great civil war , test whether nation , nation conceiv dedic , long endur .',\n",
       " 'met great battle-field war .',\n",
       " 'come dedic portion field , final rest place gave live nation might live .',\n",
       " 'altogeth fit proper .',\n",
       " ', larger sen , dedicate—w consecrate—w hallow—thi ground .',\n",
       " 'brave men , live dead , struggl , consecr , far poor power add detract .',\n",
       " 'world littl note , long rememb say , never forget .',\n",
       " 'us live , rather , dedic unfinish work fought thu far nobli advanc .',\n",
       " 'rather us dedic great task remain us—that honor dead take increa devot caus gave last full measur devotion—that high resolv dead shall die vain—that nation , god , shall new birth freedom—and govern peopl , peopl , peopl , shall perish earth .']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0bba6979-c2db-4e41-a4db-9fee596183e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d18752b3-fd49-4ce9-8836-375bec0ce670",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word,pos = 'v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) #converting words into sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0590735a-ed49-47a8-bb2b-92bda389ef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['four score seven year ago father bring forth contin , new nation , conceiv liberti , dedic proposit men creat equal .',\n",
       " 'engag great civil war , test whether nation , nation conceiv dedic , long endur .',\n",
       " 'meet great battle-field war .',\n",
       " 'come dedic portion field , final rest place give live nation might live .',\n",
       " 'altogeth fit proper .',\n",
       " ', larger sen , dedicate—w consecrate—w hallow—thi grind .',\n",
       " 'brave men , live dead , struggl , consecr , far poor power add detract .',\n",
       " 'world littl note , long rememb say , never forget .',\n",
       " 'u live , rather , dedic unfinish work fight thu far nobli advanc .',\n",
       " 'rather u dedic great task remain us—that honor dead take increa devot caus give last full measur devotion—that high resolv dead shall die vain—that nation , god , shall new birth freedom—and govern peopl , peopl , peopl , shall perish earth .']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
